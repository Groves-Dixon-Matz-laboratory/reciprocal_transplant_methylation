#Variant Calling from Methyl-Cap data
#Groves Dixon
#last updated 8-8-16



# PREPARE READ FILES FOR MAPPING
#start with 62 .trim files

#pool genotypes
mkdir genotype_catted
>do_catting
#originating from Keppels
for file in KK*2m.trim
	do ori=${file:0:1}
	rep=${file/KK/KO}
	out=${file/_2m.trim/}
	out2=${out/KK/$ori}.fastq
	echo "cat $file $rep > ./genotype_catted/$out2" >> do_catting
	done

#originating from Orpheus
for file in OO*2m.trim
	do ori=${file:0:1}
	rep=${file/OO/OK}
	out=${file/_2m.trim/}
	out2=${out/OO/$ori}.fastq
	echo "cat $file $rep > ./genotype_catted/$out2" >> do_catting
	done

#launch the job
launcher_creator.py -n do_catting -j do_catting -t 1:00:00 -N 1 -w 48 -q normal -a $allo -e $email


#add the ub files
for file in *ub.trim
	do ori=${file:0:1}
	out=${file/_2ub.trim/}
	out2=$ori${out:2}.fastq
	echo "cat $file >> ./genotype_catted/$out2"
	done

#and the timepoint 3 files
for file in *3m*.trim
	do ori=${file:0:1}
	out=${file/_3m.trim/}
	out2=$ori${out:2}.fastq
	echo "cat $file >> ./genotype_catted/$out2"
	done

#set up variable for ref genome

#In case you foolishly want to use the millepora reference:
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"


#For the digitifera reference (use this one)
GENOME_REF="/work/02260/grovesd/lonestar/adig_genome/70779_ref_Adig_1.1_chrUn.fa"

GENOME_REF="amil_longest_isogroups.fasta"

#map the reads
#note changes in mapping parameters from normal
module load bowtie
>snpmaps; for file in *.fastq; do echo "bowtie2 --end-to-end --very-sensitive -x $GENOME_REF -U $file -S $file.sam --no-unal -k 5" >> snpmaps; done
launcher_creator.py -n snpmaps -j snpmaps -q normal -t 15:00:00 -q normal -a $allo -e $email -N 2 -w 24
sbatch snpmaps.slurm


#should get 22 sam files

#CONVERT TO BAM AND SORT

#build commands
samSort.py *.sam > consort

#paste into commands file
launcher_creator.py -n consort -j consort -q normal -N 1 -w 1 -t 8:00:00 -a $allo -e $email
sbatch consort.slurm

#use picard to mark duplicates
module load java
>removeDups;for file in *Sorted.bam; do echo "java -Xms4g -jar /work/02260/grovesd/lonestar/picard/picard-tools-1.119/MarkDuplicates.jar\
 INPUT=$file\
 OUTPUT=${file/_Sorted.bam/}_dupsRemoved.bam\
 METRICS_FILE=${file/_Sorted.bam/}_dupMetrics.txt\
 AS=true\
 CREATE_INDEX=true\
 REMOVE_DUPLICATES=true" >> removeDups; done
launcher_creator.py -n removeDups -j removeDups -t 12:00:00 -q normal -N 4 -a $allo -w 2 -e $email
sbatch removeDups.slurm


#CALL SNPS WITH MPILEUP
#make list of bam files
ls *Sorted.bam > bams.txt
cat bams.txt

GENOME_REF="/work/02260/grovesd/lonestar/adig_genome/70779_ref_Adig_1.1_chrUn.fa"
GENOME_REF="/scratch/02260/grovesd/recip_meth/snp_calling/run7_transcriptome/amil_longest_isogroups.fasta"


#run mpileup
echo "samtools mpileup -B -f amil_longest_isogroups.fasta -o pileup_results.vcf -v -t DP -u -b bams.txt" > runmpile
launcher_creator.py -n runmpile -j runmpile -q normal -t 48:00:00 -a $allo -e $email
sbatch runmpile.slurm


#check stats on the results
vcftools --vcf pileup_results.vcf
		#After filtering, kept 30 out of 30 Individuals
		#After filtering, kept 310075700 out of a possible 310075700 Sites
		#Run Time = 853.00 seconds


#make variant calls from the pileup results
echo "bcftools call -c -o raw_calls.vcf --threads 48 pileup_results.vcf" > doCalls
launcher_creator.py -n doCalls -j doCalls -q normal -t 5:00:00 -a $allo -e $email
sbatch doCalls.slurm


#check the results
vcftools --vcf raw_calls.vcf

		After filtering, kept 22 out of 22 Individuals
		After filtering, kept 367354041 out of a possible 367354041 Sites

#do initial filtering to cut away mess from the vcf
echo "vcftools --vcf raw_calls.vcf --remove-indels --non-ref-af 0.1 --min-meanDP 1.0 --recode --recode-INFO-all --out recipMeth_filtered" > filter
launcher_creator.py -n filtRecip -j filter -q normal -t 5:00:00 -N 1 -w 24 -a $allok -e $email
sbatch filtRecip.slurm


#check the results
vcftools --vcf recipMeth_filtered.recode.vcf 

		#results
		After filtering, kept 22 out of 22 Individuals
		After filtering, kept 6175653 out of a possible 6175653 Sites

#identify and remove singletons
vcftools --vcf recipMeth_filtered.recode.vcf --singletons
cat out.singletons | awk '{print $1"\t"$2}' > singlesToRemove.tsv
vcftools --vcf recipMeth_filtered.recode.vcf --exclude-positions singlesToRemove.tsv --out recipMeth_noSingle --recode

		#results
		After filtering, kept 22 out of 22 Individuals
		Outputting VCF file...
		After filtering, kept 5860003 out of a possible 6175653 Sites



#get variant stats
vcfutils.pl qstats recipMeth_noSingle.recode.vcf > utilStats.tsv



vcftools --vcf recipMeth_noSingle.recode.vcf --minDP 5 --min-meanDP 5.0 --max-missing 0.8 --out recipMeth_final_mindp5_maxMiss8 --recode

	#results
	After filtering, kept 22 out of 22 Individuals
	Outputting VCF file...
	After filtering, kept 22911 out of a possible 5860003 Sites




#once you have the final vcf, analyze using the following R scripts:
adegenet_snps.R  -- build PCA from SNP data and run DAPC on it. Saves file 'snp.dapc.Rdata' which is input into DAPC_gbm_transplant.R


















---- stuff below this is old from GATK methods





#Start the pre-processing step with the sam files generated by bowtie2
module load samtools
module load gatk/3.5.0
module load java
PICARD_DIR="/work/02260/grovesd/lonestar/picard/picard-tools-1.119"
> s2b
for file in *.sam
do echo "samtools import $GENOME_REF $file ${file/.sam/.unsorted.bam} && samtools sort ${file/.sam/.unsorted.bam} -o ${file/.sam/.sorted.bam} &&\
 java -Xmx5g -jar $PICARD_DIR/AddOrReplaceReadGroups.jar INPUT=${file/.sam/.sorted.bam} OUTPUT=${file/.sam/.bam} RGID=group1 RGLB=lib1 RGPL=illumina RGPU=unit1 RGSM=${file/.sam/.bam} &&\
 samtools index ${file/.sam/.bam}"
done >> s2b
launcher_creator.py -n s2b -j s2b -q normal -t 5:00:00 -N 3 -a $allo -e $email
sbatch s2b.slurm

rm *sorted*

#use picard to mark duplicates
#make sure you allow enough nodes, or you'll get memory allocation errors and samples will drop. 
#Don't use more then wayness = 2
module load java
>removeDups;for file in *bam; do echo "java -Xms4g -jar /work/02260/grovesd/lonestar/picard/picard-tools-1.119/MarkDuplicates.jar\
 INPUT=$file\
 OUTPUT=${file/.bam/}_dupsRemoved.bam\
 METRICS_FILE=${file/.bam/}_dupMetrics.txt\
 AS=true\
 CREATE_INDEX=true" >> removeDups; done
launcher_creator.py -n removeDups -j removeDups -t 8:00:00 -q normal -N 4 -a tagmap -w 2 -e $email
sbatch removeDups.slurm

#GATHER THE REMOVAL METRIC DATA
>removalMetrics.tsv;for file in *_dupMetrics.txt; do pct=$(grep "lib1" $file | cut -f 8)
 echo -e "${file/.fastq_dupMetrics.txt/}\t$pct" >> removalMetrics.tsv; done


# step one: finding places to realign:
module load gatk/3.5.0
module load java
GENOME_REF="/work/02260/grovesd/lonestar/adig_genome/70779_ref_Adig_1.1_chrUn.fa"
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"

#build commands file
>intervals;for bam in *dupsRemoved.bam
do echo "java -Xmx5g -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar \
-T RealignerTargetCreator \
-R $GENOME_REF \
-I $bam \
-o ${bam/_dupsRemoved.bam/.intervals}" >> intervals ;done

#launch
launcher_creator.py -j intervals -n intervals -q normal -t 4:00:00 -N 5 -w 1 -a $allo -e $email
sbatch intervals.slurm


# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ls *.intervals | wc -l

# step two: realigning
module load gatk/3.5.0
module load java
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"


>realign; for bam in *dupsRemoved.bam
do echo "java -Xmx5g -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar \
-T IndelRealigner \
-R $GENOME_REF \
-targetIntervals ${bam/_dupsRemoved.bam/.intervals} \
-I $bam \
-o ${bam/.bam/.real.bam} \
-LOD 0.4" >> realign
done
launcher_creator.py -j realign -n realign -q normal -t 10:00:00 -e $email -a $allo -N 4 -w 2
sbatch realign.slurm



# did it run for all files? is the number of *.intervals files equal the number of *.bam files?
# if not, rerun the chunk above
ll *.real.bam | wc -l

#----------------------------------
# launching GATK UnifiedGenotyper for round 1 (about 30 min)
# note: it is a preliminary run needed for base quality recalibration,
#set up the initial launcher file
########################################################
module load gatk/3.5.0
module load java
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"
echo '#!/bin/bash
#SBATCH -n 1
#SBATCH -N 1
#SBATCH -p normal
#SBATCH -o unig.o%j
#SBATCH -e unig.e%j
#SBATCH -t 1:00:00
#SBATCH -A tagmap
#SBATCH --mail-type=ALL
#SBATCH --mail-user=grovesdixon@gmail.com' > unig

#add the command
echo "java -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar -nt 2 -nct 24 \
-T UnifiedGenotyper \
-R $GENOME_REF -nt 12 -nct 1 \\" >> unig

#add the input file information
for bam in *real.bam
do echo "-I $bam \\" >> unig
done
echo "-o round1.vcf" >> unig

sbatch unig
#----------------------------------

# base quality score recalibration (BQSR)

# creating high-confidence (>75 quality percentile) snp sets for 
# base quality recalibration using Kyle Hernandez's tool. (ignore the warnings)
GetHighQualVcfs.py  -i round1.vcf --percentile 75 -o .

# recalibrating quality scores
# step one: creating recalibration reports
module load gatk/3.5.0
module load java
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"

#setup commands
> bqsr
for bam in *real.bam
do echo "java -Xmx20g -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar \
-T BaseRecalibrator \
-R $GENOME_REF \
-knownSites ${bam/_dupsRemoved.real.bam/.bam_HQ.vcf} \
-I $bam \
-o ${bam/.real.bam/.recalibration_report.grp}" >> bqsr
done
launcher_creator.py -n bqsr -j bqsr -t 4:00:00 -q normal -N 5 -w 4 -e $email -a $allo
sbatch bqsr.slurm


# did it run for all files? is the number of *.grp files equal the number of *.real.bam files?
# if not, rerun the chunk above
ls *.real.bam | wc -l
ls *.grp | wc -l


# step two: rewriting bams according to recalibration reports
module load gatk/3.5.0
module load java
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"

#bulid command file
> bqsr2
for bam in *.real.bam
do echo "java -Xmx10g -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar \
-T PrintReads \
-R $GENOME_REF \
-I $bam \
-BQSR ${bam/.real.bam/.recalibration_report.grp} \
-o ${bam/.bam/.recal.bam}" >> bqsr2
done

#launch
launcher_creator.py -j bqsr2 -n bqsr2 -q normal -t 5:00:00 -N 5 -w 2 -e $email -a $allo
sbatch bqsr2.slurm


# did it run for all files? is the number of *.recal.bam files equal the number of *.real.bam files?
# if not, rerun the chunk above
ls *.real.bam | wc -l
ls *.recal.bam | wc -l

ls *.recal.bam > bams

#----------------------------------

# Second iteration of UnifiedGenotyper (on quality-recalibrated files)
# this time FOR REAL! 
# in you need indels, run the same process separately with --genotype_likelihoods_model INDEL
# I do not recommend indel tracing for 2bRAD since the tags are too short for confident indels. 
# If you still want to try, note that the subsequent recalibration stages would 
# have do be done separately for indels 

module load gatk/3.5.0
module load java
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"
echo '#!/bin/bash
#SBATCH -n 1
#SBATCH -N 1
#SBATCH -p normal
#SBATCH -o unig2.o%j
#SBATCH -e unig2.e%j
#SBATCH -t 48:00:00
#SBATCH -A tagmap
#SBATCH --mail-type=ALL
#SBATCH --mail-user=grovesdixon@gmail.com' > unig2

#add the command
echo "java -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar \
-T UnifiedGenotyper \
-R $GENOME_REF -nt 1 -nct 48 \\" >> unig2

#add the input file information
for bam in *recal.bam
do echo "-I $bam \\" >> unig2
done
echo "-o round2.vcf" >> unig2

sbatch unig2

#clean up the sample names in the VCF header
grep "FORMAT" round2.vcf
sed -i.bak 's/.fastq.bam//g' round2.vcf
sed -i.bak2 's/.trim.bam//g' round2.vcf
grep "FORMAT" round2.vcf
mv round2.vcf round2.names.vcf

#----------------------------------
# variant quality score recalibration (VQSR)


#build a clone-pairs table
#save a single-column table of all samples names from vcf as samples.txt






# extracting SNPs that are consistently genotyped in replicates 
# and have not too many heterozygotes:
replicatesMatch.pl vcf=round2.names.vcf replicates=clonepairs.tab matching=1.0>vqsr.vcf

#Results:
		1527870 total SNPs
		139963 pass hets and match filters
		77759 show non-reference alleles
		70981 have alterantive alleles in at least 2 replicate pair(s)
		70981 have matching heterozygotes in at least 0 replicate pair(s)
		58263 polymorphic
		70981 written


# determining transition-transversion ratio for true snps (will need it for tranche calibration)
vcftools --vcf vqsr.vcf --TsTv-summary
#Ts/Tv ratio: 1.61  # put your actual number into the next code chunk, --target_titv

# Recalibrating genotype calls: VQSR
# step one - creating recalibration models (30 sec)
module load gatk/3.5.0
module load java
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"


#iteration 1
echo "java -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar -T VariantRecalibrator \
-R $GENOME_REF -input round2.names.vcf -nt 48 \
-resource:repmatch,known=true,training=true,truth=true,prior=10  vqsr.vcf \
-an DP \
-an QD \
-an FS \
-an SOR \
-an MQRankSum \
-an ReadPosRankSum \
-an MQ \
-an InbreedingCoeff \
-an HaplotypeScore \
-mode SNP --maxGaussians 4 \
--target_titv 1.764 -tranche 75.0 -tranche 85.0 -tranche 90.0 -tranche 95.0 -tranche 99.0 -tranche 100 \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -rscriptFile recalibrate_SNP_plots.R" > do_vqsr
launcher_creator.py -n do_vqsr -j do_vqsr -t 12:00:00 -N 1 -q normal -a $allo -e $email
sbatch do_vqsr.slurm


#Run the plotting script on TACC
echo '#!/usr/bin/env Rscript' > dorecal.R
cat recalibrate_SNP_plots.R >> dorecal.R
load Rstats
chmod u+x dorecal.R
dorecal.R


#alter the tranches output for uploading into R
tail -n 7 recalibrate_SNP.tranches > recalibrate_SNP.tranches.csv

#run plot_tranche_results.R to look at tranches


#decide the set of filter parameters to use
#then move the plots file and rerun with just the ones you want
#*!NOTE THAT I HAD A LARGE NUMBER OF THEM, THEN REMOVED THEM AND TI/TV GOT MUCH WORSE.
#SHOULD STAY WITH LARGE NUMBER OF FILTERING PARAMS
mv recalibrate_SNP_plots.R.pdf round1_recalibrate_SNP_plots.R.pdf



# applying recalibration:
module load gatk/3.5.0
module load java
GENOME_REF="/work/02260/grovesd/genome/amil_genome_fold_c.fasta"
echo "java -jar $TACC_GATK_DIR/GenomeAnalysisTK.jar -T ApplyRecalibration \
-R $GENOME_REF -input round2.names.vcf -nt 48 \
--ts_filter_level 90.0 -mode SNP \
-recalFile round2.recal -tranchesFile recalibrate_SNP.tranches -o gatk_after_vqsr.vcf" > do_recalibration
launcher_creator.py -n do_recalibration -j do_recalibration -q normal -N 1 -w 1 -t 24:00:00 -e $email -a $allo
sbatch do_recalibration.slurm


#----------------------------------

#best to run these next steps in an idev session

# discarding loci with too many heterozygotes, which are likely lumped paralogs
#setting maximum het frequency to 0.5
# this step can also filter for the fraction of missing genotypes (default maxmiss=0.5)
hetfilter.pl vcf=gatk_after_vqsr.vcf maxhet=0.5 >hetfilt.vcf

		Results:
		527256 total loci
		433420 dropped because fraction of missing genotypes exceeded 0.5
		610 dropped because fraction of heterozygotes exceeded 0.5
		93226 written
		
# applying filter and selecting polymorphic biallelic loci genotyped in 90% or more individuals
# for parametric (GATK-based) recalibration"
vcftools --vcf hetfilt.vcf --remove-filtered-all --max-missing 0.85  --min-alleles 2 --max-alleles 2 --recode --recode-INFO-all --out filt0



#decided to go with 	--max-missing 0.85
			
# genotypic match between pairs of replicates 
# (the most important one is the last one, Heterozygote Discovery Rate)	
repMatchStats.pl vcf=filt0.recode.vcf replicates=clonepairs.tab 
		
		Round2 Results using maxMissing 0.8:
		pair	gtyped	match	[ 00	01	11 ]	HetMatch	HomoHetMismatch	HetNoCall	HetsDiscoveryRate
		K1:KK1_2m	15492	11352(73.3%)	 [90%	6%	3% ]	712	1037	390	0.50	
		K1:KO1_2m	15492	13629(88.0%)	 [87%	9%	4% ]	1251	864	104	0.72	
		K10:KK10_2m	15584	14256(91.5%)	 [90%	8%	3% ]	1082	736	63	0.73	
		K10:KO10_2m	15584	13868(89.0%)	 [90%	7%	3% ]	909	891	92	0.65	
		K10:KO10_3m	15584	13556(87.0%)	 [91%	6%	3% ]	792	922	163	0.59	
		K14:KK14_2m	15206	13247(87.1%)	 [90%	6%	4% ]	815	677	69	0.69	
		K14:KO14_2m	15206	13504(88.8%)	 [90%	6%	4% ]	870	632	46	0.72	
		K15:KK15_2m	14264	12290(86.2%)	 [91%	5%	4% ]	626	487	64	0.69	
		K15:KO15_2m	14264	12393(86.9%)	 [91%	5%	4% ]	670	463	52	0.72	
		K2:KK2_2m	15675	13986(89.2%)	 [87%	10%	3% ]	1385	1070	90	0.70	
		K2:KO2_2m	15675	13581(86.6%)	 [87%	9%	3% ]	1278	1130	151	0.67	
		K2:KO2_3m	15675	13742(87.7%)	 [89%	8%	3% ]	1083	1149	172	0.62	
		K3:KK3_2m	15569	13205(84.8%)	 [89%	8%	3% ]	1060	954	146	0.66	
		K3:KO3_2m	15569	13402(86.1%)	 [88%	8%	3% ]	1101	925	159	0.67	
		K4:KK4_2m	15636	13473(86.2%)	 [91%	6%	2% ]	862	1169	51	0.59	
		K4:KO4_2m	15636	12994(83.1%)	 [93%	5%	2% ]	682	1402	73	0.48	
		K6:KK6_2m	15588	13734(88.1%)	 [90%	7%	3% ]	943	883	114	0.65	
		K6:KO6_2m	15588	13908(89.2%)	 [89%	7%	3% ]	1022	837	91	0.69	
		K6:KO6_3m	15588	14112(90.5%)	 [90%	7%	3% ]	1032	881	70	0.68	
		K7:KK7_2m	15547	13106(84.3%)	 [91%	6%	3% ]	797	968	189	0.58	
		K7:KO7_2m	15547	13990(90.0%)	 [89%	7%	4% ]	1048	851	69	0.69	
		K7:KO7_3m	15547	14025(90.2%)	 [89%	7%	3% ]	1032	830	87	0.69	
		K8:KK8_2m	15494	14217(91.8%)	 [88%	8%	4% ]	1161	662	61	0.76	
		K8:KO8_2m	15494	13991(90.3%)	 [88%	8%	4% ]	1111	743	61	0.73	
		K9:KK9_2m	15286	13676(89.5%)	 [90%	7%	4% ]	903	606	62	0.73	
		K9:KO9_2m	15286	13569(88.8%)	 [90%	6%	3% ]	856	633	73	0.71	
		O1:OK1_2m	15539	12582(81.0%)	 [89%	7%	4% ]	876	1071	211	0.58	
		O1:OO1_2m	15539	13953(89.8%)	 [87%	9%	4% ]	1280	776	95	0.75	
		O10:OK10_2m	15461	14208(91.9%)	 [91%	6%	3% ]	911	670	24	0.72	
		O10:OO10_2m	15461	14449(93.5%)	 [90%	7%	3% ]	995	582	23	0.77	
		O14:OK14_2m	15595	14225(91.2%)	 [89%	8%	3% ]	1122	748	66	0.73	
		O14:OK14_3m	15595	14410(92.4%)	 [89%	8%	3% ]	1156	789	38	0.74	
		O14:OO14_2m	15595	8234(52.8%)	 [96%	2%	2% ]	171	789	885	0.17	
		O15:OK15_2m	15408	10333(67.1%)	 [93%	4%	3% ]	370	889	488	0.35	
		O15:OK15_3m	15408	13239(85.9%)	 [91%	6%	3% ]	755	916	116	0.59	
		O15:OO15_2m	15408	13987(90.8%)	 [88%	8%	4% ]	1103	601	65	0.77	
		O2:OK2_2m	15601	12963(83.1%)	 [89%	8%	4% ]	985	1039	246	0.61	
		O2:OO2_2m	15601	14340(91.9%)	 [85%	10%	4% ]	1496	686	52	0.80	
		O3:OK3_2m	15513	13786(88.9%)	 [87%	9%	5% ]	1185	857	94	0.71	
		O3:OO3_2m	15513	13240(85.3%)	 [88%	7%	4% ]	988	933	171	0.64	
		O4:OK4_2m	15216	14080(92.5%)	 [89%	7%	4% ]	1013	425	23	0.82	
		O4:OO4_2m	15216	12312(80.9%)	 [92%	5%	3% ]	589	739	155	0.57	
		O6:OK6_2m	15216	12258(80.6%)	 [92%	4%	3% ]	520	744	173	0.53	
		O6:OO6_2m	15216	14237(93.6%)	 [89%	7%	4% ]	984	431	16	0.81	
		O7:OK7_2m	15201	13238(87.1%)	 [91%	5%	3% ]	726	584	69	0.69	
		O7:OO7_2m	15201	13407(88.2%)	 [91%	5%	3% ]	722	575	71	0.69	
		O8:OK8_2m	15216	13162(86.5%)	 [92%	5%	3% ]	664	603	89	0.66	
		O8:OO8_2m	15216	13502(88.7%)	 [91%	5%	4% ]	736	569	60	0.70	
		O9:OK9_2m	15047	13542(90.0%)	 [90%	6%	4% ]	791	479	42	0.75	
		O9:OO9_2m	15047	12385(82.3%)	 [92%	5%	4% ]	589	637	111	0.61




#some results from running iterations of maxMissing Argument:
		maxmis	meanGenotyped	meanMatch	meanDiscov	correctCalls
		0.9	139547	0.858268667	0.695833333	119768.8176
		0.85	182269	0.830778167	0.671666667	151425.1057
		0.8	197656	0.821323917	0.661666667	162339.6001

#based on these I decided to go with 0.8
		
#check how these results are relative to non-clone pairs
nano sanity_clone_pairs.tab 
#paste this:
K14	KK1_2m
K14	OK15_2m
KK14_2m	OK1_2m
K1	KO14_2m
K1	OO15_2m
KK1_2m	OO1_2m
O15	OO1_2m
O15	KO14_2m
OK15_2m	KO1_2m
O1	OO15_2m
O1	KO1_2m
OK1_2m	KK14_2m

repMatchStats.pl vcf=filt0.recode.vcf replicates=sanity_clone_pairs.tab 

		Sanity Results:
		pair	gtyped	match	[ 00	01	11 ]	HetMatch	HomoHetMismatch	HetNoCall	HetsDiscoveryRate
		K14:KK1_2m	208249	101561(48.8%)	 [81%	3%	16% ]	3502	41291	9016	0.12	
		K14:OK15_2m	208249	97374(46.8%)	 [83%	3%	15% ]	2714	36677	10509	0.10	
		KK14_2m:OK1_2m	190763	108665(57.0%)	 [82%	3%	15% ]	3540	40620	5085	0.13	
		K1:KO14_2m	210002	115885(55.2%)	 [80%	5%	15% ]	6183	55107	4871	0.17	
		K1:OO15_2m	210002	119531(56.9%)	 [79%	6%	15% ]	7104	59188	3194	0.19	
		KK1_2m:OO1_2m	165151	99086(60.0%)	 [81%	3%	16% ]	3141	37848	7417	0.12	
		O15:OO1_2m	207052	120595(58.2%)	 [80%	5%	16% ]	5753	53394	3206	0.17	
		O15:KO14_2m	207052	116684(56.4%)	 [81%	4%	15% ]	4959	49749	4179	0.16	
		OK15_2m:KO1_2m	156006	93385(59.9%)	 [83%	3%	15% ]	2342	32189	8539	0.10	
		O1:OO15_2m	209275	121459(58.0%)	 [79%	5%	16% ]	6429	56056	2946	0.18	
		O1:KO1_2m	209275	116486(55.7%)	 [79%	5%	16% ]	5873	54684	4006	0.17	
		OK1_2m:KK14_2m	190796	108665(57.0%)	 [82%	3%	15% ]	3540	40620	5085	0.13
	

#removing clone pairs
nano to_remove.txt
#paste this:
KK1_2m
KO1_2m
KK10_2m
KO10_2m
KO10_3m
KK14_2m
KO14_2m
KK15_2m
KO15_2m
KK2_2m
KO2_2m
KO2_3m
KK3_2m
KO3_2m
KK4_2m
KO4_2m
KK6_2m
KO6_2m
KO6_3m
KK7_2m
KO7_2m
KO7_3m
KK8_2m
KO8_2m
KK9_2m
KO9_2m
OK1_2m
OO1_2m
OK10_2m
OO10_2m
OK14_2m
OK14_3m
OO14_2m
OK15_2m
OK15_3m
OO15_2m
OK2_2m
OO2_2m
OK3_2m
OO3_2m
OK4_2m
OO4_2m
OK6_2m
OO6_2m
OK7_2m
OO7_2m
OK8_2m
OO8_2m
OK9_2m
OO9_2m
KK1_2ub
KK2_2ub
KK3_2ub
KO1_2ub
KO2_2ub
KO3_2ub
OK1_2ub
OK2_2ub
OK3_2ub
OO1_2ub
OO2_2ub
OO3_2ub
#remove the clonepairs
vcftools --vcf filt0.recode.vcf --remove to_remove.txt --recode --out recip_snps_4-28-17

		After filtering, kept 22 out of 84 Individuals
		Outputting VCF file...
		After filtering, kept 15767 out of a possible 15767 Sites
		Run Time = 2.00 seconds

#Now you have a final set of variants saved as recip_snps_4-28-17.recode.vcf



		
#--------------------- DOWNSTREAM ANALYSES -------------------------

#look at allele frequencies

#option to ignore private alleles
vcftools --vcf recip_snps_4-28-17.recode.vcf --singletons

head out.singletons

cat out.singletons | awk '{print $1"\t"$2}' > singletonSites.tsv

cat singletonSites.tsv | wc -l
		4653

vcftools --vcf recip_snps_4-28-17.recode.vcf --exclude-positions singletonSites.tsv --recode --out recip_snps_4-28-17_noSingletons
		Results:
			After filtering, kept 22 out of 22 Individuals
			Outputting VCF file...
			After filtering, kept 3713 out of a possible 5381 Sites
			Run Time = 0.00 seconds



#output frequencies
vcftools --vcf recip_snps_1-27-17_noSingletons.recode.vcf --freq


#reformat them for R
cat out.frq | wc -l

#remove the header
grep -v 'CHROM' out.frq > out2.frq


cat out2.frq | awk 'BEGIN{print "chrom\tpos\tref\talt"} {split($5,ref,":");split($6,alt,":"); print $1"\t"$2"\t"ref[2]"\t"alt[2]}' > frq.tsv


###### PCA ON SNP DATA #######
#send the vcf 'recip_snps_1-27-17_noSingletons.recode.vcf' to Mac
#remove the 'c' before the chromosome numbers because SNPrelate doesn't like those
sed -i.bak 's/c//' recip_snps_1-27-17_noSingletons.recode.vcf

#build PCA with snpRelate_recip_snps.R


################## ANALYZE CPG POLYMORPHISMS #################


#remove the header bits
grep -v "##" recip_snps_1-27-17_noSingletons.recode.vcf > recip_snps_1-27-17.noHeads.vcf

#gather cpg polymorhpisms and counts for each sample
cpg_polymorphism.py -vcf recip_snps_1-27-17.noHeads.vcf -g /work/02260/grovesd/lonestar/amil_genome/amil_genome_fold_c.fasta

#now get gene-wise cpg counts
gene-wise_cpg_counts.py -i scores_cpgPolymorphisms.tsv -gff amil_moya_contigs.gff

#look at results with cpg_polymorphs.R


		
		
		
		
		
		
