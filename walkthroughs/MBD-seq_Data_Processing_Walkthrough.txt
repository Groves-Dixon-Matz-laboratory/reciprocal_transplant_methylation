#MBD-seq_Data_Processing_WalkthroughV2.txt
#Groves Dixon
#Last updated 11/16/16

########################################################
############ Information on sequencing jobs ############
########################################################
#sequencing for this project was performed in three jobs:
#job1-JA16231  -- 96 total raw fastq files representing 24 distinct samples
#job2-JA16404  -- 56 total raw fastq files representing 28 distinct samples
#job3-JA16672  -- 10 total raw fastq files representing 10 distinct samples
#62 unique samples in total
#All files are backed up here:


###############################################################
########### ORGANIZE RAW READS FROM SEQUENCING RUNS ###########
###############################################################
#Recommended to do this for each sequencing job alone

#--------- JOB1 ---------
#concatenate lane duplicates
ngs_concat.pl fastq "(.+)_S(.+)_L00"

#check how many files you have
ls *.fq | wc -l
	#24 *.fq files

#--------- JOB2 ---------
#concatenate lane duplicates
ngs_concat.pl fastq "(.+)_S(.+)_L00"

#check how many files you have
ls *.fq | wc -l
	#28 *.fq files 
	
#rename them 
for file in *.fastq; do mv $file ${file/m/};done
for file in *.fastq; do mv $file ${file/.fastq/}m.fastq;done

#get the raw read counts for each sample type
echo '>rawReadCounts.tsv;for file in *.fastq; do count=$(grep @D00289 $file | wc -l); echo -e "$file\t$count" >> readCounts.tsv;done' > getCounts
launcher_creator.py -n getCounts -j getCounts -q normal -t 1:00:00 -a $allo
sbatch getCounts.slurm

#--------- JOB3 ---------
#rename them 
for file in *.fastq; do echo "mv $file ${file/_S*.fastq/}.fastq";done



#now assemble all the files togather into a single direcotry
ls *.fastq | wc -l
#62 total files

#get raw read counts for all samples
echo '>rawReadCounts.tsv;for file in *.fastq; do count=$(grep @D00289 $file | wc -l); echo -e "$file\t$count" >> readCounts.tsv;done' > getCounts


####################################
###### NOTES ON READ TRIMMING ######
####################################

#GSAF uses NEB-Next
#The relevent sequences are shown below:

NEBNext Adapter:
5 패-/5Phos/GAT CGG AAG AGC ACA CGT CTG AAC TCC AGT C/ideoxyU/A CAC TCT TTC CCT ACA CGA CGC TCT TCC GAT C*T-3 패

NEBNext Adapter showing secondary structure:

A-CACTCTTTCCCTACACGAC-
|                     |GCTCTTCCGATCT-3'
|                     |CGAGAAGGCTAG -5'
U-CTGACCTCAAGTCTGCACA-


NEBNext Index primer 1
5패CAAGCAGAAGACGGCATACGAGATCGTGATGTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT3패


#*See NEB-Next Schematic powerpoint to show the final product


#the result is that we want to trim this from both the forward and reverse reads
GATCGGAAGAGC


#generic cutadapt command for paired end read trimming:
cutadapt -a ADAPTER_FWD -A ADAPTER_REV -o out.1.fastq -p out.2.fastq reads.1.fastq reads.2.fastq


#############################
###### TRIM AND FILTER ######
#############################

#trim the reads
mkdir trimmed
>trim1; for file in *.fastq; do echo "cutadapt -g GATCGGAAGAGC --minimum-length 15 $file > trimmed/${file/.fastq/}.trim0" >> trim1; done
launcher_creator.py -n trim1 -j trim1 -a $allo -e $email -q normal -t 5:00:00 -N 2 -w 48
sbatch trim1.slurm


#now quality filter the reads
module load fastx_toolkit
mkdir filtered
>filt; for file in *.trim0; do echo "fastq_quality_filter -Q33 -q 20 -p 90 -i $file -o ../filtered/${file/trim0/}trim" >> filt; done
launcher_creator.py -j filt -n filt -t 1:00:00 -a tagmap -e $email -N 1 -w 48 -q normal
sbatch filt.slurm

#now we have the entire set of trimmed and cleaned reads
ls *.trim | wc -l
	#62 files

#get the filtered read counts for all samples
echo '>filteredReadCounts.tsv;for file in *.trim; do count=$(grep "^@" $file | wc -l); echo -e "$file\t$count" >> filteredReadCounts.tsv;done' > getCounts


#look at read counts with read_counts.R

###################################################
##################### MAPPING #####################
###################################################
#begin with the trimmed reads
#run in its own directory
#Note if you want to try to call SNPs from these, use GATK_Walkthrough.txt

#Get the A. digitifera reference genome from NCBI: ftp://ftp.ncbi.nih.gov/genomes/Acropora_digitifera
70779_ref_Adig_1.1_chrUn.fa
ref_Adig_1.1_scaffolds.gff3


#build a symbiodinium-Adigitifera reference combo by concatenating symbiont transcritomes to the reference
clean_seq_definitions.py -i 70779_ref_Adig_1.1_chrUn.fa -delimit "|" -pos 4 -o adigitifera_ref_genome_cleaned_definitions.fa
cat adigitifera_ref_genome_cleaned_definitions.fa SymC_denovo_contigs_Ahyac_Ofu.fasta SymD_denovo_contigs_Ahyac_Ofu.fasta > adig_zoox_combo.fa
bowtie2-build adig_zoox_combo.fa adig_zoox_combo.fa


#map the reads
GENOME_REF="/work/02260/grovesd/lonestar/adig_genome/adig_zoox_combo.fa"
>maps; for file in *.trim; do echo "bowtie2 --local -x $GENOME_REF -U $file -S $file.sam --no-unal -k 5" >> maps; done
launcher_creator.py -n maps -j maps -q normal -t 48:00:00 -q normal -a $allo -N 4 -w 24
sbatch maps.slurm



#get alignment rates
grep "overall alignment rate" maps.e503490 | awk '{split($1, a, "%"); print a[1]}' > adig_zoox_combo_mapping_efficiencies.txt

#NOTE
#alignment rate summaries:
#A.millepora
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  61.93   81.15   84.84   83.61   87.98   91.78
standard error = 0.8422528

#A.digitifera
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  58.71   76.06   79.60   78.35   82.67   84.99 
standard error = 0.7422231

#FIRST CONVERT TO BAM FILES AND SORT FOR MARKING DUPLICATES
samSort.py *.sam > convertSort
launcher_creator.py -n convertSort -j convertSort -t 5:00:00 -a tagmap -N 1 -w 48 -q normal
sbatch convertSort.slurm


#NOW USE PICARD TO REMOVE DUPLICATES IN THE SORTED BAM FILES

>removeDups;for file in *Sorted.bam; do echo "java -Xms4g -jar /work/02260/grovesd/lonestar/picard/picard-tools-1.119/MarkDuplicates.jar\
 INPUT=$file\
 OUTPUT=${file/.bam/}_dupsRemoved.bam\
 METRICS_FILE=${file/.bam/}_dupMetrics.txt\
 REMOVE_DUPLICATES=true" >> removeDups; done
launcher_creator.py -n removeDups -j removeDups -t 24:00:00 -q normal -N 4 -w 2 -a $allo
sbatch removeDups.slurm


#GATHER THE REMOVAL METRIC DATA
>metRemovalMetrics.tsv;for file in *m.trim_Sorted_dupMetrics.txt; do pct=$(grep "Unknow" $file | cut -f 8); echo -e "$file\t$pct" >> metRemovalMetrics.tsv; done
 
>ubRemovalMetrics.tsv;for file in *ub.trim_Sorted_dupMetrics.txt; do pct=$(grep "Unknow" $file | cut -f 8)
 echo -e "$file\t$pct" >> ubRemovalMetrics.tsv ; done


#CONVERT BACK
>convertback;for file in *dupsRemoved.bam; do echo "samtools view -h -o ${file/.bam/}.sam $file" >> convertback; done
launcher_creator.py -n convertback -j convertback -q normal -t 3:00:00 -N 1 -w 48 -a $allo
sbatch convertback.slurm


#original step to fix names, no longer necessary because of the clean_seq_definitions.py step above
			#note the full names given in the genome do not match the contig names in the gff
			#change out the sam files like so:
			>fixNames
			for file in *_dupsRemoved.sam; do echo "cat $file | awk '{OFS=\"\t\"; split(\$3, a, \"|\"); \$3=a[4]; print}' > ${file/.trimSorted_dupsRemoved.sam/_final.sam}" >>fixNames; done
			launcher_creator.py -n fixNames -j fixNames -a $allo -q normal -N 1 -w 48 -t 00:30:00
			sbatch fixNames.slurm



#get promoter/gbm counts
module load htseq
#run htseq
>doCounts; for file in *dupsRemoved.sam; do echo "htseq-count\
 -f sam -t CDS -m intersection-nonempty --stranded=no --secondary-alignments ignore -i gene $file ref_Adig_1.1_scaffolds_promoters_up1000_down200.gff3 > ${file/.trim_Sorted_dupsRemoved.sam/CDS_counts.gff}"\
 >> doCounts; done
 
launcher_creator.py -n doCounts -j doCounts -q normal -t 5:00:00 -a $allo -e $email -N 1 -w 48
sbatch doCounts.slurm

#assemble the gene counts into a single table
assemble_htseq_counts.py -i *exon_counts.gff -o CDS_counts_11-14-16.tsv -pos 1 -delim .

#-------------- or do for full genes ---------------#
ln -s /work/02260/grovesd/stampede2/adig_genome/ref_Adig_1.1_scaffolds.gff3 .
module load htseq
#run htseq
>doCounts; for file in *dupsRemoved.sam; do echo "htseq-count\
 -f sam -t gene -m intersection-nonempty --stranded=no --secondary-alignments ignore -i gene $file ref_Adig_1.1_scaffolds.gff3 > ${file/trim_Sorted_dupsRemoved.sam/GENE_counts.gff}"\
 >> doCounts; done

#assemble the gene counts into a single table
assemble_htseq_counts.py -i *GENE_counts.gff -o GENE_counts_3-6-18.tsv -pos 1 -delim .
#---------------------------------------------------#

#assemble the gene counts into a single table
assemble_htseq_counts.py -i *exon_counts.gff -o CDS_counts_11-14-16.tsv -pos 1 -delim .

#clean lines
sed -i.bak 's/_exon_counts//g' CDS_counts_11-14-16.tsv



##################################################################
################# REMAPPING GENE EXPRESSION READS ################
##################################################################
#The gene expression reads were stored on the SRA database
#The data can be download as .sra files via FTP
#Then you unpack them using the sra toolki
#first get the sra toolkit
wget "ftp://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/current/sratoolkit.current-centos_linux64.tar.gz"
tar -xzf sratoolkit.current-centos_linux64.tar.gz
#path to scripts:
/work/02260/grovesd/lonestar/sratoolkit.2.8.0-centos_linux64/bin


#now download one sra file
#The generic path is:
ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByRun/sra/{SRR|ERR|DRR}/<first 6 characters of accession>/<accession>/<accession>.sra

Example to download SRR1640975.sra:
wget ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/SRR164/SRR1640975/SRR1640975.sra


#To get all the 'run' names go here and download the SraRunTable: https://www.ncbi.nlm.nih.gov/Traces/study/?acc=SRP049522
#print out wget commands form this:
cat SraRunTable.txt | awk '{x = substr($6, 1, 6);print "wget ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/"x"/"$6"/"$6".sra"}'


#now you should have 54 .sra files
#unpack the SOLiD data using the sra toolkit
>unpackSRA;for file in *.sra; do echo "/work/02260/grovesd/lonestar/sratoolkit.2.8.0-centos_linux64/bin/abi-dump $file" >> unpackSRA; done
launcher_creator.py -n unpackSRA -j unpackSRA -q normal -N 1 -t 1:00:00 -a $allo -e $email
sbatch unpackSRA.slurm


#now we need to map them
#download shrimp from here:
http://compbio.cs.toronto.edu/shrimp/
wget http://compbio.cs.toronto.edu/shrimp/releases/SHRiMP_2_2_3.lx26.x86_64.tar.gz


# assumes the quality files have the same filename but extension 'qual'
#attempted with wayness 12 and did not complete
GENOME_REF="/work/02260/grovesd/lonestar/adig_genome/adigitifera_ref_genome_cleaned_definitions.fa"

>do_cs_maps;for file in *.csfasta; do echo "gmapper-cs -N 3 --strata --local $file $GENOME_REF > ${file/.csfasta/.sam}">>do_cs_maps; done
launcher_creator.py -n do_cs_maps -j do_cs_maps -q normal -N 1 -w 4 -a $allo -e $email -t 5:00:00
sbatch do_cs_maps.slurm

#rename the sam files
rename _F3.sam .sam *_F3.sam
rename _F5-RNA.sam .sam *_F5-RNA.sam

#get the sample IDs back
cat SraRunTable.txt | awk '{x = substr($6, 1, 6);print "wget ftp://ftp-trace.ncbi.nih.gov/sra/sra-instant/reads/ByRun/sra/SRR/"x"/"$6"/"$6".sra"}'


#fix the names
>fixNames
for file in *.sam; do echo "cat $file | awk '{OFS=\"\t\"; split(\$3, a, \"|\"); \$3=a[4]; print}' > ${file/.sam/_final.sam}" >>fixNames; done



#now get counts


module load htseq
ln -s /work/02260/grovesd/lonestar/adig_genome/ref_Adig_1.1_scaffolds.gff3 .
>doCounts; for file in *final.sam; do echo "htseq-count\
 -f sam -t CDS -m intersection-nonempty --stranded=no -i gene $file ref_Adig_1.1_scaffolds.gff3 > ${file/.sam/_tagseq_counts.gff}"\
 >> doCounts; done

launcher_creator.py -n doCounts -j doCounts -q normal -t 5:00:00 -a $allo -e $email -N 1 -w 48
sbatch doCounts.slurm


#assemble the promoter counts into a single table
assemble_htseq_counts.py -i *_counts.gff -o all_rnaseq_counts.txt -pos 1 -delim .

#also run symbiont typing on the sam files



#once you have RNAseq read counts mapped to the digitifera genome use the following scripts:
DESeq_RNA_environment_11-4-16.R
correlation_with_expression.R

###################################################################
############### MAP RNASEQ TO TRANSCRIPTOME #######################
###################################################################
#set reference as Moya transcriptome
GENOME_REF="/work/02260/grovesd/lonestar/amillepora_transcriptome_july2014/amil.fasta"

#do mapping
>do_cs_maps;for file in *.csfasta; do echo "gmapper-cs -N 3 --strata --local $file $GENOME_REF > ${file/.csfasta/.sam}">>do_cs_maps; done
launcher_creator.py -n do_cs_maps -j do_cs_maps -q normal -N 1 -w 4 -a $allo -e $email -t 5:00:00
sbatch do_cs_maps.slurm


####################################################################################
################### LOOK AT METHYLATION DISTRIBUTION AROUDN TSSs ###################
####################################################################################
#goal here is to see if methylation decreases around promoters

#now build bed files giving windows surounding the transcription start sites of each gene
#find out how many genes there are
cut -f 3 ref_Adig_1.1_scaffolds.gff3 | grep "gene" | wc -l

#multiply this by number of windows to make for each gene
#then divide by 48 so you can output 48 files for paralleling multicoving
tss_bed_windows.py -i ref_Adig_1.1_scaffolds.gff3 -w 100 -b 3100 -c contig_sizes.tsv -m 100000


#make sure you have the right number of bed files
llt tss*_sub*.bed
ls tss*_sub*.bed | wc -l


#run multicov on this
module load bedtools
> threadMulticov
for file in tss_windows_w100_b3100_sub*.bed; do echo "bedtools multicov -bams *.bam -bed $file > ${file/tss_windows_w100_b3100/multicovCounts}" >> threadMulticov; done
launcher_creator.py -n threadMulticov -j threadMulticov -q normal -t 5:00:00 -a $allo -e $email -N 4 -w 12
sbatch threadMulticov.slurm


#build a header for the multicov output
ll *.bam | awk 'BEGIN{printf "contig\tstart\tstrop\twindow\tgenBank\tlocusName\tdirection"}
{printf "\t" $9} END{printf "\n"}' > final_thread_multicov_w100_b3100.tsv 

#remove junk from sample names
sed -i.bak 's/.trimSorted.bam//g' final_thread_multicov_w100_b3100.tsv 

#assemble counts table by catting all the threaded multicov outputs back together
for file in *multicovCounts_sub*.bed; do cat $file >> final_thread_multicov_w100_b3100.tsv; done


#run DESeq on the windows
echo "module load Rstats" > runTssDesq
echo "deseq_tss_windows.R final_thread_multicov_w100_b3100.tsv" >> runTssDesq



#reformat the rowname data
cat tss_window_deseq_results.tsv | awk '{split($1, a, "_"); print a[3]}' > window_numbers.tsv
cat tss_window_deseq_results.tsv | awk '{split($1, a, "_"); print a[1]"_"a[2]}' > loci.tsv

#then build the figures in an R idev session
dat=read.table("tss_window_deseq_results.tsv", header = T)
wdat = read.table("window_numbers.tsv")
ldat = read.table("loci.tsv", header = T)
loci=ldat$baseMean_
windows = wdat$V1
dat$windows = windows
dat$loci = loci
rdat = na.omit(dat)
mns = tapply(rdat$log2FoldChange, rdat$window, mean)
xs=as.numeric(names(mns))
library(plotrix)
sterrs = tapply(rdat$log2FoldChange, rdat$window, std.error)
stwin = sterrs/2
bottoms = mns - stwin
tops = mns + stwin
stdvs = tapply(rdat$log2FoldChange, rdat$window, sd)
mdat=data.frame(mns, bottoms, tops, xs)

ploting_data = data.frame(mns, xs, sterrs, stdvs)



pdf("tss_window_means.pdf")
plot(mdat$mns~mdat$xs, xlab = 'Gene Position', ylab = 'MBD-score', main = "50bp windows upstream and downstream")
dev.off()

pdf("tss_window_mean_lines.pdf")
plot(mdat$mns~mdat$xs, xlab = 'Gene Position', ylab = 'MBD-score', main = "50bp windows upstream and downstream", pch = 26)
lines(mdat$mns~mdat$xs, lwd = 2)
lines(mdat$tops~mdat$xs, col='red')
lines(mdat$bottoms~mdat$xs, col='red')
dev.off()



load("mbd_classes.Rdata")
load("all_annotations.Rdata")
colnames(all.anot) = c("geneID", "gene", "locusName", "loci")
c2=merge(classes, all.anot, by = 'gene')

comp1=rdat[rdat$loci %in% c2[c2$mbd.class == 1,'loci'],]
comp2=rdat[rdat$loci %in% c2[c2$mbd.class == 2,'loci'],]

mns1 = tapply(comp1$log2FoldChange, comp1$window, mean)
mns2 = tapply(comp2$log2FoldChange, comp2$window, mean)
x1=as.numeric(names(mns1))
x2=as.numeric(names(mns2))
sterrs1 = tapply(comp1$log2FoldChange, comp1$window, std.error)
sterrs2 = tapply(comp2$log2FoldChange, comp2$window, std.error)
stdvs1 = tapply(comp1$log2FoldChange, comp1$window, sd)
stdvs2 = tapply(comp2$log2FoldChange, comp2$window, sd)

#write out data to plot on Mac
out = data.frame(mns, xs, sterrs, stdvs)
out1 = data.frame(mns1, x1, sterrs1, stdvs1)
out2 = data.frame(mns2, x2, sterrs2, stdvs2)

save(out)



pdf("tss_indiv_windows.pdf")
plot(1, 1, xlim=c(-3000, 3000), ylim=c(-5,3), type="n", xlab = 'Gene Position', ylab = 'MBD-score', main = "100bp windows upstream and downstream")
lineNum = 0
for (i in sample(unique(rdat$loci), size= 100)){
	lineNum = lineNum + 1
	if lineNum/10
	sub = rdat[rdat$loci == i,]
	xs = as.numeric(sub$windows)
	mbd.scores = sub$log2FoldChange
	lines(mbd.scores~xs, col=alpha('grey', 0.001))
}
lines(mdat$mns~mdat$xs, lwd = 2)
dev.off()


####################################################################
#################### LOOK AT METHYLATION ALONG EXONS  ##############
####################################################################
#add numbered exon tags to the gff
number_exons_in_gff.py -i ref_Adig_1.1_scaffolds.gff3 -o ref_Adig_1.1_scaffolds_numbered_exons.gff3 

#get the counts with htseq
>doExonCounts; for file in *_dupsRemoved.sam; do echo "htseq-count\
 -f sam -t CDS -m intersection-nonempty --stranded=no -i EXON $file ref_Adig_1.1_scaffolds_numbered_exons.gff3 > ${file/.trimSorted_dupsRemoved.sam/_exon_counts.gff}"\
 >> doExonCounts; done

launcher_creator.py -n doExonCounts -j doExonCounts -q normal -t 6:00:00 -a $allo -e $email -N 1 -w 48
sbatch doExonCounts.slurm









